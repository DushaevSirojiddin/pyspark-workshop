# ğŸ“š Advanced Analytics: Joins, Aggregations & Window Functions

## ğŸ¯ **Theory: Distributed Data Relationships & Analytics**

This exercise explores the **heart of distributed analytics** - combining datasets and extracting insights through sophisticated SQL operations. In production environments, these patterns form the backbone of:

- **Data Warehousing**: Star schema fact/dimension joins
- **Business Intelligence**: Multi-table aggregations and KPIs
- **Advanced Analytics**: Ranking, percentiles, and comparative metrics

### **ğŸ§  Distributed Join Fundamentals:**

**The Challenge**: When tables are partitioned across multiple machines, joining them requires sophisticated coordination:
- **Shuffle Operations**: Moving data across the network
- **Join Strategies**: Broadcast vs Sort-Merge vs Hash joins
- **Data Locality**: Minimizing network overhead

**Spark's Solution**: Catalyst optimizer automatically chooses optimal join strategy based on table sizes and data distribution.

## ğŸ”§ **Code Walkthrough: Enterprise-Grade Join Patterns**

### **ğŸ—ï¸ Realistic Data Modeling:**

```python
# Employee fact table (larger dataset)
employees_data = [
    (1, "Alice", 1, 85000, "2020-01-15"),  # emp_id, name, dept_id, salary, hire_date
    # ... 8 employees
]

# Department dimension table (smaller, reference data)
dept_data = [
    (1, "Engineering", "Technology"),      # dept_id, dept_name, division
    # ... 4 departments
]
```

**ğŸ“Š Data Architecture Analysis:**
- **Fact Table**: Employee records (transactional data)
- **Dimension Table**: Department lookup (reference data)
- **Foreign Key**: `dept_id` links the tables
- **Realistic Ratios**: 8:4 employee-to-department ratio mirrors real businesses

### **ğŸ”— Join Operations Deep Dive:**

```python
joined_df = employees_df.join(dept_df, "dept_id", "inner")
```

**ğŸ’¡ Join Strategy Breakdown:**

#### **ğŸ¯ Join Key Selection:**
```python
.join(dept_df, "dept_id", "inner")
```
**Syntax Analysis:**
- **Single column join**: Spark auto-detects common column name
- **Join type**: "inner" excludes unmatched records
- **Implicit optimization**: Catalyst chooses optimal strategy

#### **âš¡ Under the Hood - Join Execution:**

```python
# What Spark Actually Does:
# 1. Analyze table sizes
employees_df.count()  # 8 records
dept_df.count()       # 4 records

# 2. Choose strategy: Broadcast Join (dept_df is small)
# 3. Broadcast dept_df to all worker nodes
# 4. Hash join locally on each partition
```

**ğŸ¯ Join Strategy Decision Tree:**
- **Broadcast Join**: If one table < 10MB (configurable)
- **Sort-Merge Join**: For large tables with sorted data
- **Hash Join**: Default for medium-sized tables

#### **ğŸ“Š Join Type Implications:**

```python
# Different join types for different business needs:
.join(dept_df, "dept_id", "inner")     # Only employees with departments
.join(dept_df, "dept_id", "left")      # All employees, null for missing depts
.join(dept_df, "dept_id", "right")     # All departments, null for empty depts
.join(dept_df, "dept_id", "outer")     # Everything, nulls where unmatched
```

## ğŸ“Š **Aggregation Patterns - Business Intelligence Foundation**

### **ğŸ¯ Multi-Dimensional Aggregations:**

```python
dept_summary = joined_df \
    .groupBy("dept_name", "division") \
    .agg(
        F.sum("salary").alias("total_salary"),
        F.avg("salary").alias("avg_salary"),
        F.count("emp_id").alias("employee_count"),
        F.max("salary").alias("max_salary"),
        F.min("salary").alias("min_salary")
    ) \
    .withColumn("avg_salary_rounded", F.round(F.col("avg_salary"), 2)) \
    .orderBy(F.desc("total_salary"))
```

**ğŸ§  Aggregation Architecture Analysis:**

#### **1. Grouping Strategy:**
```python
.groupBy("dept_name", "division")  # Multi-level grouping
```
**Business Logic**: Groups by department within division
- Creates hierarchical summary (Division â†’ Department â†’ Metrics)
- Enables roll-up analysis (department totals â†’ division totals)

#### **2. Multiple Aggregation Functions:**
```python
F.sum("salary").alias("total_salary"),    # Financial totals
F.avg("salary").alias("avg_salary"),      # Central tendency
F.count("emp_id").alias("employee_count"), # Volume metrics
F.max("salary").alias("max_salary"),      # Range analysis
F.min("salary").alias("min_salary")       # Range analysis
```

**ğŸ’° Business Intelligence Patterns:**
- **Financial Metrics**: Sum for budgeting, average for benchmarking
- **Volume Metrics**: Count for capacity planning
- **Range Metrics**: Min/max for equity analysis

#### **3. Post-Aggregation Transformations:**
```python
.withColumn("avg_salary_rounded", F.round(F.col("avg_salary"), 2))
```
**ğŸ¯ Data Presentation**: Round financial figures for readability (business requirement)

#### **4. Business-Driven Sorting:**
```python
.orderBy(F.desc("total_salary"))  # Highest-budget departments first
```

### **âš¡ Distributed Aggregation Performance:**

**ğŸ”„ Execution Phases:**
1. **Map Phase**: Local pre-aggregation on each partition
2. **Shuffle Phase**: Group related data across nodes
3. **Reduce Phase**: Final aggregation by group

**ğŸ“Š Memory Optimization:**
```python
# Spark automatically optimizes:
# - Partial aggregations reduce shuffle data
# - Hash-based aggregation for performance
# - Spill-to-disk for large groups
```

## ğŸ† **Window Functions - Advanced Analytics Mastery**

### **ğŸ¯ Window Function Architecture:**

```python
window_spec = Window.partitionBy("dept_name").orderBy(F.desc("salary"))

ranked_employees = joined_df \
    .withColumn("salary_rank", F.row_number().over(window_spec)) \
    .withColumn("salary_percentile", F.percent_rank().over(window_spec))
```

**ğŸ§  Window Function Deep Dive:**

#### **ğŸ“Š Window Specification Design:**
```python
Window.partitionBy("dept_name").orderBy(F.desc("salary"))
```
**Conceptual Model**:
- **Partition**: Create separate "windows" for each department
- **Order**: Within each department, sort by salary (highest first)
- **Function**: Apply ranking within each window

#### **ğŸ¯ Ranking Functions Comparison:**

```python
# Different ranking strategies:
F.row_number().over(window_spec)    # 1, 2, 3, 4... (unique ranks)
F.rank().over(window_spec)          # 1, 2, 2, 4... (ties get same rank)
F.dense_rank().over(window_spec)    # 1, 2, 2, 3... (no gaps after ties)
F.percent_rank().over(window_spec)  # 0.0 to 1.0 (percentile ranking)
```

**ğŸ’¼ Business Applications:**
- **Performance Reviews**: Employee ranking within departments
- **Compensation Analysis**: Salary percentiles for equity assessment
- **Promotion Planning**: Top performers identification

#### **âš¡ Window Function Performance:**

**ğŸ”„ Execution Strategy:**
```python
# Spark optimization for window functions:
# 1. Partition data by window key (dept_name)
# 2. Sort within each partition (by salary desc)
# 3. Apply function locally within each partition
# 4. No shuffle required if properly partitioned
```

**ğŸ“Š Memory Management:**
- **Streaming windows**: Process large partitions incrementally
- **Bounded preceding/following**: Limit memory usage for moving averages
- **Catalyst optimization**: Combines multiple window functions efficiently

### **ğŸ¯ Advanced Window Patterns:**

```python
# Production examples you might use:
from pyspark.sql.window import Window

# Moving averages
rolling_window = Window.partitionBy("dept_name").orderBy("hire_date").rowsBetween(-2, 0)
df.withColumn("avg_last_3_salaries", F.avg("salary").over(rolling_window))

# Cumulative sums
cumulative_window = Window.partitionBy("dept_name").orderBy("hire_date").rowsBetween(Window.unboundedPreceding, 0)
df.withColumn("cumulative_salary_cost", F.sum("salary").over(cumulative_window))

# Lead/lag analysis
comparison_window = Window.partitionBy("dept_name").orderBy("hire_date")
df.withColumn("previous_hire_salary", F.lag("salary", 1).over(comparison_window))
```

## ğŸš¨ **Performance Optimization & Common Pitfalls**

### **âš¡ Join Optimization Strategies:**

#### **âœ… Broadcast Join Optimization:**
```python
from pyspark.sql.functions import broadcast

# Force broadcast for small dimension tables
joined_df = employees_df.join(broadcast(dept_df), "dept_id")
# Eliminates shuffle for small lookup tables
```

#### **ğŸ¯ Bucketing for Large Joins:**
```python
# Pre-partition large tables by join key
employees_df.write.bucketBy(10, "dept_id").saveAsTable("employees_bucketed")
dept_df.write.bucketBy(10, "dept_id").saveAsTable("dept_bucketed")

# Joins between bucketed tables avoid shuffle
```

### **ğŸ“Š Aggregation Performance:**

#### **ğŸ”„ Partial Aggregation Benefits:**
```python
# Spark automatically applies partial aggregation:
# Before shuffle: Local sum/count on each partition
# After shuffle: Combine partial results
# Result: Dramatically reduced network traffic
```

#### **âš ï¸ Data Skew Mitigation:**
```python
# Problem: One department has 80% of employees
# Solution: Salt the keys
df.withColumn("salted_dept", F.concat(F.col("dept_id"), F.lit("_"), F.rand() * 10))
```

### **ğŸš¨ Common Anti-Patterns:**

```python
# âŒ DON'T: Cartesian joins
df1.join(df2)  # Missing join condition = cartesian product!

# âŒ DON'T: Collect before aggregation
df.collect().groupBy(...)  # Brings all data to driver first

# âŒ DON'T: Multiple window functions with different specs
Window.partitionBy("dept").orderBy("salary")     # First spec
Window.partitionBy("region").orderBy("hire_date") # Second spec - requires reshuffle

# âœ… DO: Combine window functions with same spec
window_spec = Window.partitionBy("dept").orderBy("salary")
df.withColumn("rank", F.row_number().over(window_spec)) \
  .withColumn("percentile", F.percent_rank().over(window_spec))
```

## ğŸ“ **Educational Learning Progression**

This exercise builds critical distributed computing skills:

1. **Join Strategy Understanding**: How Spark chooses and executes joins
2. **Aggregation Patterns**: Multi-dimensional business analytics
3. **Window Function Mastery**: Advanced analytical operations
4. **Performance Awareness**: Understanding shuffle operations and optimization

## ğŸ’­ **Real-World Applications**

**What This Prepares You For:**

### **ğŸ¢ Data Warehousing:**
- **Star Schema Joins**: Fact tables with multiple dimensions
- **ETL Pipelines**: Complex transformations across related tables
- **Data Quality**: Referential integrity validation

### **ğŸ“Š Business Intelligence:**
- **KPI Calculations**: Revenue, headcount, productivity metrics
- **Comparative Analysis**: Department/division performance
- **Trend Analysis**: Time-based aggregations and rankings

### **ğŸ¯ Advanced Analytics:**
- **Customer Segmentation**: Percentile-based groupings
- **Performance Management**: Employee ranking and evaluation
- **Financial Analysis**: Budget allocation and variance analysis

---

**ğŸ¯ Key Takeaway**: These operations represent the **analytical engine of modern data platforms**. Mastering joins, aggregations, and window functions enables you to transform raw data into business insights at scale. Every production analytics pipeline, data warehouse, and BI system relies on these fundamental distributed computing patterns!