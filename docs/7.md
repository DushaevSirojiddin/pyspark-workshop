# üìö Data Quality Engineering: Building Trust in Production Data

## üéØ **Theory: Data Quality as the Foundation of Data-Driven Decisions**

Data Quality is the **silent guardian** of analytics and ML systems. Poor data quality leads to:
- **Incorrect business decisions** based on flawed insights
- **Model degradation** in ML systems
- **Regulatory compliance issues** in finance/healthcare
- **Lost revenue** from operational inefficiencies

### **üß† The Six Dimensions of Data Quality:**

1. **Completeness**: Are all required fields populated?
2. **Validity**: Do values conform to business rules?
3. **Consistency**: Are calculations and relationships correct?
4. **Uniqueness**: Are there unwanted duplicates?
5. **Timeliness**: Is data current and relevant?
6. **Accuracy**: Does data reflect real-world truth?

**Production Reality**: Netflix estimates that poor data quality costs them $1M+ per day in suboptimal recommendations. Data quality isn't just a "nice to have" - it's business critical.

## üîß **Code Walkthrough: Comprehensive Data Quality Framework**

### **üìä Systematic Quality Assessment Approach:**

```python
total_records = sales_df.count()
print(f"üìä Total records: {total_records:,}")
```

**üéØ Baseline Establishment**: Every quality assessment starts with understanding data volume
- **Trend Monitoring**: Compare with historical record counts
- **Completeness Context**: Baseline for percentage calculations
- **Performance Baseline**: Reference for quality check performance

### **üîç Completeness Analysis - The Foundation:**

```python
print("\nüîç Completeness Analysis:")
for column in sales_df.columns:
    null_count = sales_df.filter(F.col(column).isNull()).count()
    completeness_rate = ((total_records - null_count) / total_records) * 100
    print(f"  ‚Ä¢ {column}: {completeness_rate:.1f}% complete ({null_count} nulls)")
```

**üß† Completeness Deep Dive:**

#### **Why Completeness Matters:**
```python
# Business impact examples:
# Missing customer_segment ‚Üí Cannot run targeted campaigns
# Missing transaction_date ‚Üí Cannot analyze trends
# Missing product ‚Üí Cannot attribute revenue
```

#### **Production Completeness Patterns:**
```python
# Advanced completeness checking you'd implement:

# 1. Critical field validation
critical_fields = ["transaction_id", "total_amount", "product"]
for field in critical_fields:
    null_count = df.filter(F.col(field).isNull()).count()
    if null_count > 0:
        raise DataQualityException(f"Critical field {field} has {null_count} nulls")

# 2. Conditional completeness
# Rule: If customer_segment = "Enterprise", annual_revenue must not be null
enterprise_missing_revenue = df.filter(
    (F.col("customer_segment") == "Enterprise") & 
    F.col("annual_revenue").isNull()
).count()

# 3. Temporal completeness
# Rule: Recent data should have higher completeness
recent_completeness = df.filter(F.col("transaction_date") >= "2024-01-01") \
    .select([F.avg(F.when(F.col(c).isNull(), 0).otherwise(1)).alias(f"{c}_completeness") 
             for c in df.columns])
```

### **‚úÖ Business Rule Validation:**

```python
# Check for negative amounts
negative_amounts = sales_df.filter(F.col("total_amount") < 0).count()
print(f"  ‚Ä¢ Negative amounts: {negative_amounts} records")

# Check for invalid quantities
invalid_quantities = sales_df.filter(F.col("quantity") <= 0).count()
print(f"  ‚Ä¢ Invalid quantities: {invalid_quantities} records")
```

**üíº Business Logic Enforcement:**

#### **Domain-Specific Validation Rules:**
```python
# Financial data validation
F.col("total_amount") < 0  # Revenue cannot be negative

# Inventory validation  
F.col("quantity") <= 0     # Cannot sell zero or negative items

# Date validation
F.col("transaction_date") > F.current_date()  # Future transactions invalid
```

#### **Cross-Field Consistency:**
```python
validation_df = sales_df \
    .withColumn("calculated_amount", F.col("quantity") * F.col("unit_price")) \
    .withColumn("amount_diff", F.abs(F.col("total_amount") - F.col("calculated_amount")))

inconsistent_amounts = validation_df.filter(F.col("amount_diff") > 0.01).count()
```

**üéØ Mathematical Consistency Checking:**
- **Business Rule**: total_amount = quantity √ó unit_price
- **Tolerance**: 0.01 for floating-point precision
- **Production Impact**: Prevents revenue calculation errors

#### **Advanced Validation Patterns:**
```python
# Referential integrity
orphaned_sales = sales_df.join(products_df, "product", "left_anti")  # Sales without products

# Range validation
price_outliers = sales_df.filter(
    (F.col("unit_price") < 1) |      # Suspiciously cheap
    (F.col("unit_price") > 10000)    # Suspiciously expensive
)

# Pattern validation
invalid_transaction_ids = sales_df.filter(
    ~F.col("transaction_id").rlike("^TXN_[0-9]{6}$")  # Must match pattern
)
```

### **üìà Statistical Outlier Detection:**

```python
# Calculate percentiles
percentiles = sales_df.select(
    F.expr("percentile_approx(total_amount, 0.25)").alias("q1"),
    F.expr("percentile_approx(total_amount, 0.75)").alias("q3"),
    F.avg("total_amount").alias("mean"),
    F.stddev("total_amount").alias("stddev")
).collect()[0]

# IQR method for outliers
iqr = percentiles["q3"] - percentiles["q1"]
lower_bound = percentiles["q1"] - 1.5 * iqr
upper_bound = percentiles["q3"] + 1.5 * iqr
```

**üß† Statistical Outlier Analysis:**

#### **IQR (Interquartile Range) Method:**
```python
# Mathematical foundation:
# Q1 = 25th percentile
# Q3 = 75th percentile  
# IQR = Q3 - Q1
# Outliers: < Q1 - 1.5*IQR  OR  > Q3 + 1.5*IQR
```

**üìä Why This Method Works:**
- **Robust to distribution shape**: Works for non-normal distributions
- **Business intuitive**: Values beyond "normal business range"
- **Tunable sensitivity**: Adjust 1.5 multiplier based on business needs

#### **Alternative Outlier Detection Methods:**

```python
# 1. Z-Score method (assumes normal distribution)
z_score_outliers = sales_df.filter(
    F.abs((F.col("total_amount") - percentiles["mean"]) / percentiles["stddev"]) > 3
)

# 2. Modified Z-Score (more robust)
median_val = sales_df.select(F.expr("percentile_approx(total_amount, 0.5)")).collect()[0][0]
mad = sales_df.select(F.expr(f"percentile_approx(abs(total_amount - {median_val}), 0.5)")).collect()[0][0]
modified_z_outliers = sales_df.filter(
    F.abs((F.col("total_amount") - median_val) / (1.4826 * mad)) > 3.5
)

# 3. Business-rule outliers
business_outliers = sales_df.filter(
    F.col("total_amount") > F.lit(10000)  # Transactions > $10K need review
)
```

### **üîÑ Duplicate Detection Strategies:**

```python
duplicate_ids = sales_df \
    .groupBy("transaction_id") \
    .count() \
    .filter(F.col("count") > 1) \
    .count()
```

**üéØ Multi-Level Duplicate Detection:**

#### **1. Exact ID Duplicates:**
```python
# Primary key violations
duplicate_ids = df.groupBy("transaction_id").count().filter(F.col("count") > 1)
```

#### **2. Logical Duplicates:**
```python
# Same transaction characteristics (potential data pipeline issues)
logical_duplicates = df.groupBy(
    "transaction_date", "sales_rep", "product", "total_amount"
).count().filter(F.col("count") > 1)
```

#### **3. Near-Duplicates:**
```python
# Similar but not identical (data entry errors)
# Group by rounded amounts and similar timestamps
near_duplicates = df.withColumn("rounded_amount", F.round(F.col("total_amount"), -1)) \
    .withColumn("date_hour", F.date_trunc("hour", F.col("transaction_date"))) \
    .groupBy("sales_rep", "product", "rounded_amount", "date_hour") \
    .count().filter(F.col("count") > 1)
```

### **üéØ Comprehensive Quality Scoring:**

```python
validity_rate = ((total_records - negative_amounts - invalid_quantities - inconsistent_amounts) / total_records) * 100
uniqueness_rate = ((total_records - duplicate_ids) / total_records) * 100

print(f"  ‚Ä¢ Validity Score: {validity_rate:.1f}%")
print(f"  ‚Ä¢ Uniqueness Score: {uniqueness_rate:.1f}%")
print(f"  ‚Ä¢ Outlier Rate: {(outliers/total_records)*100:.1f}%")
```

**üìä Quality Metrics Framework:**

#### **Composite Quality Score:**
```python
# Weighted quality calculation
weights = {
    "completeness": 0.25,    # Critical for analytics
    "validity": 0.30,        # Most important for business rules
    "uniqueness": 0.20,      # Important for accurate counts
    "consistency": 0.25      # Critical for financial accuracy
}

overall_quality = (
    completeness_rate * weights["completeness"] +
    validity_rate * weights["validity"] +
    uniqueness_rate * weights["uniqueness"] +
    consistency_rate * weights["consistency"]
)
```

#### **Quality Thresholds & Alerting:**
```python
# Production alerting thresholds
quality_thresholds = {
    "completeness": 95.0,    # Alert if < 95% complete
    "validity": 98.0,        # Alert if < 98% valid
    "uniqueness": 99.5,      # Alert if > 0.5% duplicates
    "overall": 96.0          # Alert if overall < 96%
}

# Generate alerts
if validity_rate < quality_thresholds["validity"]:
    send_alert(f"Data validity below threshold: {validity_rate:.1f}%")
```

## üö® **Production Data Quality Patterns**

### **üîÑ Continuous Quality Monitoring:**

```python
# Production implementation patterns:

class DataQualityMonitor:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.rules = []
    
    def add_rule(self, name, condition, severity="ERROR"):
        self.rules.append({
            "name": name,
            "condition": condition, 
            "severity": severity
        })
    
    def validate(self, df):
        results = []
        for rule in self.rules:
            violation_count = df.filter(rule["condition"]).count()
            results.append({
                "rule": rule["name"],
                "violations": violation_count,
                "severity": rule["severity"],
                "percentage": (violation_count / df.count()) * 100
            })
        return results

# Usage example:
monitor = DataQualityMonitor(spark)
monitor.add_rule("No negative amounts", F.col("total_amount") < 0, "ERROR")
monitor.add_rule("Reasonable prices", F.col("unit_price") > 10000, "WARNING")
monitor.add_rule("Valid transaction IDs", ~F.col("transaction_id").rlike("^TXN_"), "ERROR")

quality_results = monitor.validate(sales_df)
```

### **üìä Quality Trending & Reporting:**

```python
# Time-series quality tracking
quality_trend = df.withColumn("date", F.to_date(F.col("transaction_date"))) \
    .groupBy("date") \
    .agg(
        F.count("*").alias("total_records"),
        F.sum(F.when(F.col("total_amount").isNull(), 1).otherwise(0)).alias("null_amounts"),
        F.sum(F.when(F.col("total_amount") < 0, 1).otherwise(0)).alias("negative_amounts")
    ) \
    .withColumn("amount_completeness", 
               (F.col("total_records") - F.col("null_amounts")) / F.col("total_records") * 100) \
    .withColumn("amount_validity",
               (F.col("total_records") - F.col("negative_amounts")) / F.col("total_records") * 100)

# Detect quality degradation
quality_trend.withColumn("completeness_trend", 
    F.col("amount_completeness") - F.lag("amount_completeness", 1).over(
        Window.orderBy("date")
    )).filter(F.col("completeness_trend") < -5)  # Alert on 5%+ drop
```

### **üõ°Ô∏è Data Quality Gates:**

```python
# Pipeline quality gates
def quality_gate(df, min_quality_score=95.0):
    """Fail pipeline if quality below threshold"""
    
    quality_score = calculate_overall_quality(df)
    
    if quality_score < min_quality_score:
        raise DataQualityException(
            f"Quality score {quality_score:.1f}% below threshold {min_quality_score}%"
        )
    
    logger.info(f"Quality gate passed: {quality_score:.1f}%")
    return df

# Data quarantine for bad records
def quarantine_bad_data(df):
    """Separate good and bad records"""
    
    good_data = df.filter(
        F.col("total_amount").isNotNull() &
        (F.col("total_amount") >= 0) &
        (F.col("quantity") > 0)
    )
    
    bad_data = df.subtract(good_data)
    
    # Save bad data for investigation
    bad_data.write.mode("append").parquet("quarantine/bad_data")
    
    return good_data
```

## üéì **Educational Learning Progression**

This exercise builds production-critical data engineering skills:

1. **Quality Dimensions**: Understanding completeness, validity, consistency, uniqueness
2. **Statistical Methods**: Outlier detection using IQR and other techniques
3. **Business Rules**: Implementing domain-specific validation logic
4. **Automation**: Building reusable quality monitoring frameworks
5. **Production Patterns**: Quality gates, alerting, and trending
6. **Data Governance**: Establishing trust and compliance

## üí≠ **Real-World Impact Scenarios**

### **üè¶ Financial Services:**
```python
# Regulatory compliance requirements
def sox_compliance_check(df):
    # Sarbanes-Oxley: Financial data must be complete and accurate
    critical_fields = ["transaction_amount", "account_id", "transaction_date"]
    
    for field in critical_fields:
        completeness = calculate_completeness(df, field)
        if completeness < 100.0:
            raise ComplianceException(f"SOX violation: {field} not 100% complete")
```

### **üè• Healthcare:**
```python
# HIPAA and patient safety requirements
def patient_data_validation(df):
    # Patient IDs must be unique and properly formatted
    duplicate_patients = df.groupBy("patient_id").count().filter(F.col("count") > 1)
    if duplicate_patients.count() > 0:
        raise PatientSafetyException("Duplicate patient IDs detected")
```

### **üöó Autonomous Vehicles:**
```python
# Safety-critical sensor data validation
def sensor_data_quality(df):
    # Missing sensor readings could cause accidents
    sensor_completeness = calculate_completeness(df, "lidar_distance")
    if sensor_completeness < 99.9:
        raise SafetyCriticalException("Insufficient sensor data quality")
```

## ‚ö° **Performance Optimization for Quality Checks**

### **üéØ Efficient Quality Assessment:**

```python
# Optimize multiple quality checks
quality_summary = df.agg(
    F.count("*").alias("total_records"),
    F.sum(F.when(F.col("total_amount").isNull(), 1).otherwise(0)).alias("null_amounts"),
    F.sum(F.when(F.col("total_amount") < 0, 1).otherwise(0)).alias("negative_amounts"),
    F.sum(F.when(F.col("quantity") <= 0, 1).otherwise(0)).alias("invalid_quantities"),
    F.countDistinct("transaction_id").alias("unique_ids")
).collect()[0]

# Single pass through data instead of multiple filters
```

### **üìä Sampling for Large Datasets:**

```python
# Quality assessment on samples for very large datasets
sample_df = df.sample(False, 0.1, seed=42)  # 10% sample
quality_score = assess_quality(sample_df)

# Extrapolate to full dataset with confidence intervals
```

---

**üéØ Key Takeaway**: Data quality isn't just about "clean data" - it's about **building trust in your data platform**. These quality checks form the foundation that enables confident decision-making, regulatory compliance, and reliable ML systems. In production environments, robust data quality engineering is the difference between a trusted data platform and one that nobody wants to use. Master these patterns, and you'll be building systems that stakeholders can rely on for critical business decisions!