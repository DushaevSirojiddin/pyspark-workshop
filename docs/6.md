# 📚 File Operations & Data Partitioning: Production-Scale Data Processing

## 🎯 **Theory: The Data Storage & Performance Foundation**

This exercise tackles **production-critical concepts** that separate basic Spark usage from enterprise-grade data engineering:

### **🗂️ File Format Strategy:**
Different file formats serve different purposes in the data ecosystem:
- **CSV**: Human-readable, schema-on-read, but inefficient for analytics
- **Parquet**: Columnar, compressed, optimized for analytical queries
- **Delta Lake**: ACID transactions, time travel, schema evolution

### **🧩 Data Partitioning Fundamentals:**
**Partitioning** is the art of organizing data for optimal query performance:
- **Physical Layout**: How data is stored on disk/cloud storage
- **Query Optimization**: Partition pruning eliminates irrelevant data scans
- **Parallel Processing**: Each partition can be processed independently

**Production Reality**: Poor partitioning strategy can make queries 100x slower. Getting this right is crucial for scalable analytics.

## 🔧 **Code Walkthrough: Enterprise Data Pipeline Patterns**

### **📋 Schema-First File Reading:**

```python
sales_schema = StructType([
    StructField("transaction_id", StringType(), True),
    StructField("transaction_date", StringType(), True),
    StructField("product", StringType(), True),
    # ... complete schema definition
])

sales_df = spark.read.csv("../data/workshop_sales.csv", header=True, schema=sales_schema)
```

**🛡️ Why Explicit Schemas Are Production-Critical:**

#### **1. Performance Benefits:**
```python
# Without schema (slow):
df = spark.read.csv("file.csv", header=True)
# Spark must scan entire file to infer types

# With schema (fast):
df = spark.read.csv("file.csv", header=True, schema=schema)
# Immediate type knowledge, no scanning required
```

#### **2. Data Quality Assurance:**
```python
# Schema acts as a contract:
StructField("total_amount", DoubleType(), True)  # Must be numeric
# Invalid data (e.g., "N/A") will throw clear errors
# vs silent conversion to StringType without schema
```

#### **3. Evolution Protection:**
```python
# Production benefit: New columns in source files won't break existing pipelines
# Missing columns throw descriptive errors instead of runtime failures
```

### **🔄 Date Processing & Feature Engineering:**

```python
enriched_sales = sales_df \
    .withColumn("year", F.year(F.to_date(F.col("transaction_date")))) \
    .withColumn("month", F.month(F.to_date(F.col("transaction_date")))) \
    .withColumn("revenue_category",
               F.when(F.col("total_amount") < 500, "Small")
               .when(F.col("total_amount") < 2000, "Medium")
               .otherwise("Large"))
```

**📅 Temporal Data Processing Deep Dive:**

#### **Date Parsing Strategy:**
```python
F.to_date(F.col("transaction_date"))  # Converts string to DateType
```
**🎯 Production Considerations:**
- **Format Detection**: Spark auto-detects common date formats
- **Null Handling**: Invalid dates become null (not errors)
- **Timezone Awareness**: Consider UTC vs local time in production

#### **Time-Based Partitioning Preparation:**
```python
.withColumn("year", F.year(...))   # Partition key candidate
.withColumn("month", F.month(...)) # Sub-partition for finer granularity
```
**💡 Strategic Design**: These columns become partition keys for time-series analytics optimization.

#### **Business Logic Encoding:**
```python
F.when(F.col("total_amount") < 500, "Small")
 .when(F.col("total_amount") < 2000, "Medium")
 .otherwise("Large")
```
**📊 Categorical Feature Engineering**: Creates business-meaningful segments for analysis and reporting.

### **💰 Financial Analytics Pipeline:**

```python
sales_with_products = enriched_sales \
    .join(products_df, "product", "inner") \
    .withColumn("cost_total", F.col("quantity").cast("double") * F.col("cost")) \
    .withColumn("profit", F.col("total_amount") - F.col("cost_total"))

sales_with_products.cache()  # Cache for multiple operations
```

**🧠 Enterprise Data Integration Pattern:**

#### **Reference Data Joins:**
```python
.join(products_df, "product", "inner")
```
**🎯 Design Pattern**: Enriching transactional data with reference/dimension data
- **Broadcast Join Optimization**: Small product catalog broadcasted to all nodes
- **Data Consistency**: Single source of truth for product attributes

#### **Derived Metrics Calculation:**
```python
.withColumn("cost_total", F.col("quantity").cast("double") * F.col("cost"))
.withColumn("profit", F.col("total_amount") - F.col("cost_total"))
```
**💼 Business Intelligence Foundation**: 
- **Unit Economics**: Cost per transaction
- **Profitability Analysis**: Revenue minus cost
- **Type Safety**: Explicit casting prevents precision errors

#### **Strategic Caching:**
```python
sales_with_products.cache()
```
**⚡ Performance Optimization**:
- **Multiple Downstream Operations**: Avoid recomputing expensive joins
- **Memory vs Compute Trade-off**: Cache frequently accessed DataFrames
- **Lazy Evaluation**: Cache triggered on first action

### **📊 Multi-Dimensional Business Analytics:**

```python
monthly_sales = sales_with_products \
    .groupBy("year", "month", "region") \
    .agg(
        F.sum("total_amount").alias("total_revenue"),
        F.sum("profit").alias("total_profit"),
        F.count("transaction_id").alias("transaction_count"),
        F.avg("total_amount").alias("avg_transaction_value")
    ) \
    .orderBy("year", "month", F.desc("total_revenue"))
```

**📈 Time-Series Analytics Architecture:**

#### **Hierarchical Grouping:**
```python
.groupBy("year", "month", "region")
```
**🎯 Dimensional Analysis**: Enables drill-down from annual → monthly → regional performance

#### **Comprehensive Metrics Suite:**
```python
F.sum("total_amount").alias("total_revenue"),      # Volume metrics
F.sum("profit").alias("total_profit"),            # Profitability metrics  
F.count("transaction_id").alias("transaction_count"), # Activity metrics
F.avg("total_amount").alias("avg_transaction_value")  # Intensity metrics
```

**💼 Business Intelligence Pattern**: 
- **Financial KPIs**: Revenue and profit tracking
- **Operational KPIs**: Transaction volume and average order value
- **Trend Analysis**: Month-over-month comparisons

### **🚀 Product Performance Analytics:**

```python
product_performance = sales_with_products \
    .groupBy("product", "category") \
    .agg(
        F.sum("total_amount").alias("total_revenue"),
        F.sum("profit").alias("total_profit"),
        F.sum("quantity").alias("total_quantity"),
        F.avg("profit").alias("avg_profit_per_sale")
    ) \
    .withColumn("profit_margin", F.col("total_profit") / F.col("total_revenue")) \
    .orderBy(F.desc("total_revenue"))
```

**🏆 Advanced Analytics Patterns:**

#### **Post-Aggregation Calculations:**
```python
.withColumn("profit_margin", F.col("total_profit") / F.col("total_revenue"))
```
**📊 Calculated KPIs**: Derived metrics from aggregated values
- **Financial Ratios**: Margin analysis for profitability assessment
- **Business Intelligence**: Ready-to-use metrics for dashboards

## 📂 **Data Partitioning: The Performance Game-Changer**

### **🎯 Partitioning Strategy Deep Dive:**

```python
print(f"Original partitions: {sales_with_products.rdd.getNumPartitions()}")

partitioned_by_region = sales_with_products.repartition(F.col("region"))
print(f"After repartitioning by region: {partitioned_by_region.rdd.getNumPartitions()}")
```

**🧠 Partitioning Theory & Practice:**

#### **Physical vs Logical Partitioning:**
- **Logical**: DataFrame partitions in memory (for parallel processing)
- **Physical**: File organization on storage (for query optimization)

#### **Repartitioning Benefits:**
```python
.repartition(F.col("region"))  # Data locality by region
```
**⚡ Performance Impact**:
- **Query Optimization**: Filters on region only scan relevant partitions
- **Parallel Processing**: Each region processed independently
- **Join Optimization**: Co-located data reduces shuffle overhead

### **💾 Multi-Format Output Strategy:**

```python
# Parquet with partitioning (optimized for analytics)
partitioned_by_region.write \
    .partitionBy("region", "year") \
    .mode("overwrite") \
    .parquet("output/sales_partitioned")

# CSV for reporting (human-readable)
monthly_sales.coalesce(1).write.mode("overwrite").csv("output/monthly_sales", header=True)
```

**📊 Storage Format Decision Matrix:**

| Format | Use Case | Pros | Cons |
|--------|----------|------|------|
| **Parquet + Partitioning** | Analytics, ETL | Columnar, compressed, partition pruning | Not human-readable |
| **CSV** | Reports, exports | Human-readable, universal | Large files, slow queries |
| **Delta** | Production lakes | ACID, time travel, schema evolution | Requires Delta Lake |

#### **Hierarchical Partitioning:**
```python
.partitionBy("region", "year")
```
**🗂️ Directory Structure Created:**
```
output/sales_partitioned/
├── region=North/
│   ├── year=2024/
│   │   └── part-00000.parquet
│   └── year=2023/
│       └── part-00001.parquet
├── region=South/
│   ├── year=2024/
│   │   └── part-00002.parquet
```

**🎯 Query Optimization Impact:**
```sql
-- Query: "Show North region 2024 sales"
-- Without partitioning: Scans ALL files
-- With partitioning: Scans ONLY region=North/year=2024/ files
-- Performance gain: 10-100x faster
```

## ⚡ **Performance Optimization Techniques**

### **🔄 Caching Strategy:**

```python
sales_with_products.cache()  # Strategic caching decision
```

**💡 When to Cache:**
- ✅ **Multiple downstream operations**: Aggregations + writes
- ✅ **Expensive computations**: Complex joins or transformations
- ✅ **Iterative algorithms**: ML training, graph processing

**⚠️ When NOT to Cache:**
- ❌ **Single use**: DataFrames used only once
- ❌ **Memory constraints**: Large DataFrames on small clusters
- ❌ **Simple operations**: Basic filters or projections

### **📊 Partition Management:**

```python
# Reduce partitions for small outputs
monthly_sales.coalesce(1).write.csv("output/monthly_sales")
```

**🎯 Coalesce vs Repartition:**
- **Coalesce**: Reduces partitions without shuffle (efficient)
- **Repartition**: Can increase/decrease with shuffle (flexible)

**📈 Partition Size Guidelines:**
- **Too many small partitions**: High task overhead
- **Too few large partitions**: Poor parallelism
- **Sweet spot**: 128MB - 1GB per partition

## 🚨 **Production Considerations & Best Practices**

### **✅ Data Pipeline Best Practices:**

```python
# Production enhancements you'd add:

# 1. Error handling
try:
    sales_df = spark.read.csv("sales.csv", schema=schema)
except AnalysisException as e:
    logger.error(f"Schema mismatch: {e}")
    raise

# 2. Data validation
assert sales_df.count() > 0, "Empty sales dataset"
assert sales_df.filter(F.col("total_amount") < 0).count() == 0, "Negative amounts found"

# 3. Incremental processing
latest_date = spark.sql("SELECT MAX(transaction_date) FROM sales_table")
new_data = sales_df.filter(F.col("transaction_date") > latest_date)

# 4. Resource management
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "256MB")
```

### **🔍 Monitoring & Observability:**

```python
# Production monitoring patterns:

# 1. Row count validation
input_count = sales_df.count()
output_count = enriched_sales.count()
logger.info(f"Processed {input_count} → {output_count} records")

# 2. Data quality metrics
null_count = sales_df.filter(F.col("total_amount").isNull()).count()
quality_score = (input_count - null_count) / input_count
logger.info(f"Data quality score: {quality_score:.2%}")

# 3. Performance metrics
start_time = time.time()
result.write.parquet("output")
duration = time.time() - start_time
logger.info(f"Write completed in {duration:.2f} seconds")
```

## 🎓 **Educational Learning Progression**

This exercise builds production-ready skills:

1. **Schema Management**: Type safety and performance optimization
2. **Data Integration**: Multi-table joins and reference data patterns
3. **Feature Engineering**: Business logic implementation
4. **Analytics Pipelines**: Multi-dimensional aggregations
5. **Storage Optimization**: Partitioning and format selection
6. **Performance Tuning**: Caching and resource management

## 💭 **Real-World Applications**

**What This Prepares You For:**

### **🏭 Data Engineering:**
- **ETL Pipelines**: Extract from multiple sources, transform, and load optimally
- **Data Lake Architecture**: Efficient storage and retrieval patterns
- **Performance Optimization**: Partition strategy for petabyte-scale data

### **📊 Business Intelligence:**
- **Data Warehousing**: Star schema implementation with optimized storage
- **Reporting Pipelines**: Automated metric calculation and export
- **Self-Service Analytics**: Fast query performance through proper partitioning

### **🚀 Advanced Analytics:**
- **Feature Stores**: Engineered features for ML pipelines
- **Real-time Dashboards**: Pre-aggregated metrics for low-latency queries
- **Historical Analysis**: Time-partitioned data for trend analysis

---

**🎯 Key Takeaway**: This exercise demonstrates the **complete data engineering lifecycle** - from raw file ingestion through transformation, analytics, and optimized storage. These patterns form the backbone of modern data platforms, enabling organizations to process petabytes of data efficiently and derive business value at scale. Mastering file operations and partitioning is essential for any production Spark deployment!