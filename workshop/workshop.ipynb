{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2662995e-31db-4fab-8988-6b334b368c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulletproof PySpark Workshop - All Function Conflicts Resolved\n",
    "# This version works without any import conflicts\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import builtins  # For Python built-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52939683-6e1c-45f8-9067-90aea5a25125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SPARK SESSION SETUP\n",
    "# ==========================================\n",
    "\n",
    "def create_workshop_spark():\n",
    "    \"\"\"Create Spark session with proper configuration\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PySpark Workshop - Bulletproof\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c08c8781-4136-41b2-9c6e-fe58507c877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# DATASET CREATION - SAFE VERSION\n",
    "# ==========================================\n",
    "\n",
    "def create_workshop_datasets():\n",
    "    \"\"\"Create datasets without any function conflicts\"\"\"\n",
    "    print(\"📊 Creating workshop datasets...\")\n",
    "    \n",
    "    # Create sales data (10,000 records for good performance)\n",
    "    sales_data = []\n",
    "    base_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    products = ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']\n",
    "    regions = ['North', 'South', 'East', 'West']\n",
    "    segments = ['Enterprise', 'SMB', 'Consumer']\n",
    "    \n",
    "    for i in range(10000):\n",
    "        transaction_date = base_date + timedelta(days=random.randint(0, 365))\n",
    "        quantity = random.randint(1, 5)\n",
    "        unit_price = random.uniform(50, 2000)\n",
    "        \n",
    "        sales_data.append({\n",
    "            'transaction_id': f'TXN_{i+1:06d}',\n",
    "            'transaction_date': transaction_date.strftime('%Y-%m-%d'),\n",
    "            'product': random.choice(products),\n",
    "            'region': random.choice(regions),\n",
    "            'sales_rep': f'Rep_{random.randint(1, 20):02d}',\n",
    "            'quantity': quantity,\n",
    "            'unit_price': builtins.round(unit_price, 2),  # Use Python's round\n",
    "            'total_amount': builtins.round(quantity * unit_price, 2),\n",
    "            'customer_segment': random.choice(segments)\n",
    "        })\n",
    "    \n",
    "    # Create customer data\n",
    "    customer_data = []\n",
    "    for i in range(1000):\n",
    "        customer_data.append({\n",
    "            'customer_id': f'CUST_{i+1:05d}',\n",
    "            'company_name': f'Company_{i+1}',\n",
    "            'region': random.choice(regions),\n",
    "            'industry': random.choice(['Tech', 'Healthcare', 'Finance', 'Retail']),\n",
    "            'annual_revenue': random.randint(100000, 10000000)\n",
    "        })\n",
    "    \n",
    "    # Create product catalog\n",
    "    product_catalog = [\n",
    "        {'product': 'Laptop', 'category': 'Computing', 'cost': 800, 'margin': 0.3},\n",
    "        {'product': 'Mouse', 'category': 'Accessories', 'cost': 20, 'margin': 0.5},\n",
    "        {'product': 'Keyboard', 'category': 'Accessories', 'cost': 50, 'margin': 0.4},\n",
    "        {'product': 'Monitor', 'category': 'Display', 'cost': 300, 'margin': 0.35},\n",
    "        {'product': 'Headphones', 'category': 'Audio', 'cost': 100, 'margin': 0.45}\n",
    "    ]\n",
    "    \n",
    "    # Save to CSV\n",
    "    pd.DataFrame(sales_data).to_csv('../data/workshop_sales.csv', index=False)\n",
    "    pd.DataFrame(customer_data).to_csv('../data/workshop_customers.csv', index=False)\n",
    "    pd.DataFrame(product_catalog).to_csv('../data/workshop_products.csv', index=False)\n",
    "    \n",
    "    print(\"✅ Created datasets:\")\n",
    "    print(f\"  • workshop_sales.csv: {len(sales_data):,} records\")\n",
    "    print(f\"  • workshop_customers.csv: {len(customer_data):,} records\") \n",
    "    print(f\"  • workshop_products.csv: {len(product_catalog)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72c3e17d-c72b-413d-8202-f2e822d6082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_workshop_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a214130e-d93f-4bc8-b48a-fdd9d709120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# EXERCISE 1: BASIC OPERATIONS\n",
    "# ==========================================\n",
    "\n",
    "def exercise_1_basic_spark_operations():\n",
    "    \"\"\"Basic Spark operations without conflicts\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXERCISE 1: BASIC SPARK OPERATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    spark = create_workshop_spark()\n",
    "    \n",
    "    try:\n",
    "        # Create simple test data directly in Spark\n",
    "        print(\"📊 Creating test data...\")\n",
    "        people_data = [\n",
    "            (1, \"Alice Johnson\", 28, \"Engineering\", \"alice@company.com\"),\n",
    "            (2, \"Bob Smith\", 35, \"Marketing\", \"bob@company.com\"),\n",
    "            (3, \"Carol Davis\", 42, \"Engineering\", \"carol@company.com\"),\n",
    "            (4, \"David Wilson\", 31, \"Sales\", \"david@company.com\"),\n",
    "            (5, \"Eve Brown\", 26, \"Marketing\", \"eve@company.com\"),\n",
    "            (6, \"Frank Miller\", 45, \"Engineering\", \"frank@company.com\")\n",
    "        ]\n",
    "        \n",
    "        # Create DataFrame directly (no file I/O issues)\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", IntegerType(), False),\n",
    "            StructField(\"name\", StringType(), False),\n",
    "            StructField(\"age\", IntegerType(), False),\n",
    "            StructField(\"department\", StringType(), False),\n",
    "            StructField(\"email\", StringType(), False)\n",
    "        ])\n",
    "        \n",
    "        people_df = spark.createDataFrame(people_data, schema)\n",
    "        \n",
    "        print(\"📋 Original data:\")\n",
    "        people_df.show()\n",
    "        \n",
    "        # Filter by age > 30\n",
    "        print(\"\\n🔍 Filtering: age > 30\")\n",
    "        filtered_df = people_df.filter(F.col(\"age\") > 30)\n",
    "        filtered_df.show()\n",
    "        \n",
    "        # Add calculated columns\n",
    "        print(\"\\n➕ Adding calculated columns:\")\n",
    "        enhanced_df = filtered_df \\\n",
    "            .withColumn(\"age_doubled\", F.col(\"age\") * 2) \\\n",
    "            .withColumn(\"age_category\",\n",
    "                       F.when(F.col(\"age\") < 35, \"Young Professional\")\n",
    "                       .when(F.col(\"age\") < 45, \"Mid-Career\")\n",
    "                       .otherwise(\"Senior Professional\")) \\\n",
    "            .withColumn(\"email_domain\", \n",
    "                       F.regexp_extract(F.col(\"email\"), \"@(.+)\", 1))\n",
    "        \n",
    "        enhanced_df.show()\n",
    "        \n",
    "        # Save results\n",
    "        print(\"\\n💾 Saving results...\")\n",
    "        enhanced_df.coalesce(1).write.mode(\"overwrite\").csv(\"output/exercise1\", header=True)\n",
    "        \n",
    "        print(\"✅ Exercise 1 completed successfully!\")\n",
    "        \n",
    "        return enhanced_df\n",
    "        \n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e63abb7f-a469-4474-a304-cd5abe3656ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise_1_basic_spark_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e966950-3bf3-4bff-9b78-0b1a4bd38088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# EXERCISE 2: JOINS AND AGGREGATIONS\n",
    "# ==========================================\n",
    "\n",
    "def exercise_2_joins_and_aggregations():\n",
    "    \"\"\"Advanced joins and aggregations\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXERCISE 2: JOINS AND AGGREGATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    spark = create_workshop_spark()\n",
    "    \n",
    "    try:\n",
    "        # Create employee data\n",
    "        employees_data = [\n",
    "            (1, \"Alice\", 1, 85000, \"2020-01-15\"),\n",
    "            (2, \"Bob\", 2, 75000, \"2019-03-20\"),\n",
    "            (3, \"Carol\", 1, 95000, \"2018-07-10\"),\n",
    "            (4, \"David\", 3, 65000, \"2021-02-28\"),\n",
    "            (5, \"Eve\", 2, 70000, \"2020-11-05\"),\n",
    "            (6, \"Frank\", 1, 105000, \"2017-09-12\"),\n",
    "            (7, \"Grace\", 4, 80000, \"2019-06-18\"),\n",
    "            (8, \"Henry\", 3, 68000, \"2021-08-03\")\n",
    "        ]\n",
    "        \n",
    "        # Create department data\n",
    "        dept_data = [\n",
    "            (1, \"Engineering\", \"Technology\"),\n",
    "            (2, \"Marketing\", \"Business\"),\n",
    "            (3, \"Sales\", \"Business\"),\n",
    "            (4, \"HR\", \"Operations\")\n",
    "        ]\n",
    "        \n",
    "        # Create DataFrames\n",
    "        employees_df = spark.createDataFrame(employees_data, \n",
    "                                           [\"emp_id\", \"name\", \"dept_id\", \"salary\", \"hire_date\"])\n",
    "        dept_df = spark.createDataFrame(dept_data, \n",
    "                                      [\"dept_id\", \"dept_name\", \"division\"])\n",
    "        \n",
    "        print(\"👥 Employees:\")\n",
    "        employees_df.show()\n",
    "        \n",
    "        print(\"🏢 Departments:\")\n",
    "        dept_df.show()\n",
    "        \n",
    "        # Join operations\n",
    "        print(\"\\n🔗 Joining data...\")\n",
    "        joined_df = employees_df.join(dept_df, \"dept_id\", \"inner\")\n",
    "        joined_df.show()\n",
    "        \n",
    "        # Aggregations by department\n",
    "        print(\"\\n📊 Department aggregations:\")\n",
    "        dept_summary = joined_df \\\n",
    "            .groupBy(\"dept_name\", \"division\") \\\n",
    "            .agg(\n",
    "                F.sum(\"salary\").alias(\"total_salary\"),\n",
    "                F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "                F.count(\"emp_id\").alias(\"employee_count\"),\n",
    "                F.max(\"salary\").alias(\"max_salary\"),\n",
    "                F.min(\"salary\").alias(\"min_salary\")\n",
    "            ) \\\n",
    "            .withColumn(\"avg_salary_rounded\", F.round(F.col(\"avg_salary\"), 2)) \\\n",
    "            .orderBy(F.desc(\"total_salary\"))\n",
    "        \n",
    "        dept_summary.show()\n",
    "        \n",
    "        # Window functions - ranking employees within departments\n",
    "        print(\"\\n🏆 Employee rankings within departments:\")\n",
    "        window_spec = Window.partitionBy(\"dept_name\").orderBy(F.desc(\"salary\"))\n",
    "        \n",
    "        ranked_employees = joined_df \\\n",
    "            .withColumn(\"salary_rank\", F.row_number().over(window_spec)) \\\n",
    "            .withColumn(\"salary_percentile\", F.percent_rank().over(window_spec)) \\\n",
    "            .select(\"name\", \"dept_name\", \"salary\", \"salary_rank\", \"salary_percentile\") \\\n",
    "            .orderBy(\"dept_name\", \"salary_rank\")\n",
    "        \n",
    "        ranked_employees.show()\n",
    "        \n",
    "        # Save results\n",
    "        print(\"\\n💾 Saving results...\")\n",
    "        dept_summary.coalesce(1).write.mode(\"overwrite\").csv(\"output/dept_summary\", header=True)\n",
    "        ranked_employees.coalesce(1).write.mode(\"overwrite\").csv(\"output/employee_rankings\", header=True)\n",
    "        \n",
    "        print(\"✅ Exercise 2 completed successfully!\")\n",
    "        \n",
    "        return joined_df, dept_summary\n",
    "        \n",
    "    finally:\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d71b754-ed72-4be2-a7a4-0fa6b063d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise_2_joins_and_aggregations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03a4aa2b-c5e6-4b6c-96be-8e5240e8b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# EXERCISE 3: WORKING WITH FILES\n",
    "# ==========================================\n",
    "\n",
    "def exercise_3_file_operations():\n",
    "    \"\"\"Working with CSV files and partitioning\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXERCISE 3: FILE OPERATIONS AND PARTITIONING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # First create the datasets\n",
    "    create_workshop_datasets()\n",
    "    \n",
    "    spark = create_workshop_spark()\n",
    "    \n",
    "    try:\n",
    "        # Define explicit schemas to avoid type conflicts\n",
    "        sales_schema = StructType([\n",
    "            StructField(\"transaction_id\", StringType(), True),\n",
    "            StructField(\"transaction_date\", StringType(), True),\n",
    "            StructField(\"product\", StringType(), True),\n",
    "            StructField(\"region\", StringType(), True),\n",
    "            StructField(\"sales_rep\", StringType(), True),\n",
    "            StructField(\"quantity\", IntegerType(), True),\n",
    "            StructField(\"unit_price\", DoubleType(), True),\n",
    "            StructField(\"total_amount\", DoubleType(), True),\n",
    "            StructField(\"customer_segment\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        customers_schema = StructType([\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"company_name\", StringType(), True),\n",
    "            StructField(\"region\", StringType(), True),\n",
    "            StructField(\"industry\", StringType(), True),\n",
    "            StructField(\"annual_revenue\", LongType(), True)\n",
    "        ])\n",
    "        \n",
    "        products_schema = StructType([\n",
    "            StructField(\"product\", StringType(), True),\n",
    "            StructField(\"category\", StringType(), True),\n",
    "            StructField(\"cost\", DoubleType(), True),\n",
    "            StructField(\"margin\", DoubleType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Read CSV files with explicit schemas\n",
    "        print(\"📖 Reading CSV files with explicit schemas...\")\n",
    "        sales_df = spark.read.csv(\"../data/workshop_sales.csv\", header=True, schema=sales_schema)\n",
    "        customers_df = spark.read.csv(\"../data/workshop_customers.csv\", header=True, schema=customers_schema)\n",
    "        products_df = spark.read.csv(\"../data/workshop_products.csv\", header=True, schema=products_schema)\n",
    "        \n",
    "        print(f\"📊 Sales records: {sales_df.count():,}\")\n",
    "        print(\"Sample sales data:\")\n",
    "        sales_df.limit(5).show()\n",
    "        \n",
    "        # Data transformations\n",
    "        print(\"\\n🔄 Data transformations...\")\n",
    "        enriched_sales = sales_df \\\n",
    "            .withColumn(\"year\", F.year(F.to_date(F.col(\"transaction_date\")))) \\\n",
    "            .withColumn(\"month\", F.month(F.to_date(F.col(\"transaction_date\")))) \\\n",
    "            .withColumn(\"revenue_category\",\n",
    "                       F.when(F.col(\"total_amount\") < 500, \"Small\")\n",
    "                       .when(F.col(\"total_amount\") < 2000, \"Medium\")\n",
    "                       .otherwise(\"Large\"))\n",
    "        \n",
    "        # Join with products for cost analysis\n",
    "        print(\"\\n🔗 Joining with product catalog...\")\n",
    "        sales_with_products = enriched_sales \\\n",
    "            .join(products_df, \"product\", \"inner\") \\\n",
    "            .withColumn(\"cost_total\", F.col(\"quantity\").cast(\"double\") * F.col(\"cost\")) \\\n",
    "            .withColumn(\"profit\", F.col(\"total_amount\") - F.col(\"cost_total\"))\n",
    "        \n",
    "        sales_with_products.cache()  # Cache for multiple operations\n",
    "        \n",
    "        # Aggregations\n",
    "        print(\"\\n📊 Monthly sales analysis:\")\n",
    "        monthly_sales = sales_with_products \\\n",
    "            .groupBy(\"year\", \"month\", \"region\") \\\n",
    "            .agg(\n",
    "                F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "                F.sum(\"profit\").alias(\"total_profit\"),\n",
    "                F.count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "                F.avg(\"total_amount\").alias(\"avg_transaction_value\")\n",
    "            ) \\\n",
    "            .orderBy(\"year\", \"month\", F.desc(\"total_revenue\"))\n",
    "        \n",
    "        monthly_sales.show(20)\n",
    "        \n",
    "        # Product performance\n",
    "        print(\"\\n🏆 Product performance:\")\n",
    "        product_performance = sales_with_products \\\n",
    "            .groupBy(\"product\", \"category\") \\\n",
    "            .agg(\n",
    "                F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "                F.sum(\"profit\").alias(\"total_profit\"),\n",
    "                F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "                F.avg(\"profit\").alias(\"avg_profit_per_sale\")\n",
    "            ) \\\n",
    "            .withColumn(\"profit_margin\", F.col(\"total_profit\") / F.col(\"total_revenue\")) \\\n",
    "            .orderBy(F.desc(\"total_revenue\"))\n",
    "        \n",
    "        product_performance.show()\n",
    "        \n",
    "        # Partitioning demonstration (safe version)\n",
    "        print(\"\\n📂 Partitioning analysis:\")\n",
    "        print(f\"Original partitions: {sales_with_products.rdd.getNumPartitions()}\")\n",
    "        \n",
    "        # Repartition by region for better performance\n",
    "        partitioned_by_region = sales_with_products.repartition(F.col(\"region\"))\n",
    "        print(f\"After repartitioning by region: {partitioned_by_region.rdd.getNumPartitions()}\")\n",
    "        \n",
    "        # Save partitioned data\n",
    "        print(\"\\n💾 Saving partitioned data...\")\n",
    "        partitioned_by_region.write \\\n",
    "            .partitionBy(\"region\", \"year\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .parquet(\"output/sales_partitioned\")\n",
    "        \n",
    "        # Save summary reports\n",
    "        monthly_sales.coalesce(1).write.mode(\"overwrite\").csv(\"output/monthly_sales\", header=True)\n",
    "        product_performance.coalesce(1).write.mode(\"overwrite\").csv(\"output/product_performance\", header=True)\n",
    "        \n",
    "        print(\"✅ Exercise 3 completed successfully!\")\n",
    "        \n",
    "        return sales_with_products, monthly_sales, product_performance\n",
    "        \n",
    "    finally:\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6de4dc9d-cfb0-4c46-a784-fd43bb595b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXERCISE 3: FILE OPERATIONS AND PARTITIONING\n",
      "============================================================\n",
      "📊 Creating workshop datasets...\n",
      "✅ Created datasets:\n",
      "  • workshop_sales.csv: 10,000 records\n",
      "  • workshop_customers.csv: 1,000 records\n",
      "  • workshop_products.csv: 5 records\n",
      "📖 Reading CSV files with explicit schemas...\n",
      "📊 Sales records: 10,000\n",
      "Sample sales data:\n",
      "+--------------+----------------+--------+------+---------+--------+----------+------------+----------------+\n",
      "|transaction_id|transaction_date| product|region|sales_rep|quantity|unit_price|total_amount|customer_segment|\n",
      "+--------------+----------------+--------+------+---------+--------+----------+------------+----------------+\n",
      "|    TXN_000001|      2024-11-08|   Mouse| South|   Rep_16|       2|      73.9|      147.79|             SMB|\n",
      "|    TXN_000002|      2024-04-29|Keyboard| South|   Rep_18|       1|   1430.16|     1430.16|      Enterprise|\n",
      "|    TXN_000003|      2024-02-24|   Mouse|  East|   Rep_13|       4|     89.81|      359.23|             SMB|\n",
      "|    TXN_000004|      2024-08-11|Keyboard| North|   Rep_10|       4|    732.09|     2928.36|      Enterprise|\n",
      "|    TXN_000005|      2024-03-15|Keyboard| North|   Rep_01|       5|     757.1|     3785.49|             SMB|\n",
      "+--------------+----------------+--------+------+---------+--------+----------+------------+----------------+\n",
      "\n",
      "\n",
      "🔄 Data transformations...\n",
      "\n",
      "🔗 Joining with product catalog...\n",
      "\n",
      "📊 Monthly sales analysis:\n",
      "+----+-----+------+-----------------+------------------+-----------------+---------------------+\n",
      "|year|month|region|    total_revenue|      total_profit|transaction_count|avg_transaction_value|\n",
      "+----+-----+------+-----------------+------------------+-----------------+---------------------+\n",
      "|2024|    1|  East|687192.0099999997| 505932.0099999999|              230|   2987.7913478260853|\n",
      "|2024|    1| North|634498.9599999996| 447038.9599999998|              218|   2910.5456880733927|\n",
      "|2024|    1| South|603009.1700000003| 446069.1700000003|              199|   3030.1968341708557|\n",
      "|2024|    1|  West|592278.2199999999|         429038.22|              191|   3100.9330890052347|\n",
      "|2024|    2|  East|        719939.54|         553519.54|              205|   3511.9001951219516|\n",
      "|2024|    2|  West|633692.2399999995| 463072.2399999997|              189|    3352.868994708992|\n",
      "|2024|    2| North|        619607.91|449887.91000000015|              210|   2950.5138571428574|\n",
      "|2024|    2| South|534738.9500000002|401588.95000000036|              182|      2938.1260989011|\n",
      "|2024|    3| South|673521.1799999999|518271.18000000005|              217|   3103.7842396313363|\n",
      "|2024|    3|  East|667709.7099999996|499029.70999999973|              208|   3210.1428365384595|\n",
      "|2024|    3| North|637783.0900000003|         454813.09|              217|    2939.092580645163|\n",
      "|2024|    3|  West|611215.7299999999|462035.73000000004|              196|   3118.4476020408156|\n",
      "|2024|    4| North|        753630.26| 600290.2600000004|              235|   3206.9372765957446|\n",
      "|2024|    4| South|596194.2499999999| 430054.2499999997|              207|   2880.1654589371974|\n",
      "|2024|    4|  West|559290.4499999997|405710.44999999995|              185|     3023.19162162162|\n",
      "|2024|    4|  East|530744.5199999999| 365054.5200000001|              184|   2884.4810869565213|\n",
      "|2024|    5| South|709963.8200000004|         549953.82|              209|    3396.956076555026|\n",
      "|2024|    5| North|680115.5399999999| 525645.5399999998|              205|   3317.6367804878046|\n",
      "|2024|    5|  West|633801.6099999999| 482391.6100000004|              203|   3122.1754187192114|\n",
      "|2024|    5|  East|618352.0799999998|         473182.08|              209|    2958.622392344497|\n",
      "+----+-----+------+-----------------+------------------+-----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "🏆 Product performance:\n",
      "+----------+-----------+-----------------+------------------+--------------+-------------------+------------------+\n",
      "|   product|   category|    total_revenue|      total_profit|total_quantity|avg_profit_per_sale|     profit_margin|\n",
      "+----------+-----------+-----------------+------------------+--------------+-------------------+------------------+\n",
      "|  Keyboard|Accessories|6412965.649999995| 6101965.649999997|          6220|  2962.119247572814|0.9515044962076168|\n",
      "|     Mouse|Accessories|6224626.120000019| 6104546.120000016|          6004| 3055.3283883883964|0.9807088815159227|\n",
      "|Headphones|      Audio|6205066.780000002|        5608866.78|          5962|  2837.059575113809|0.9039172306861133|\n",
      "|    Laptop|  Computing|6149940.849999995|1257140.8499999992|          6116|  621.1170207509878|0.2044151123827476|\n",
      "|   Monitor|    Display|6024652.690000001| 4271152.689999994|          5845|  2200.490824317359|0.7089458778411329|\n",
      "+----------+-----------+-----------------+------------------+--------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "📂 Partitioning analysis:\n",
      "Original partitions: 1\n",
      "After repartitioning by region: 1\n",
      "\n",
      "💾 Saving partitioned data...\n",
      "✅ Exercise 3 completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataFrame[product: string, transaction_id: string, transaction_date: string, region: string, sales_rep: string, quantity: int, unit_price: double, total_amount: double, customer_segment: string, year: int, month: int, revenue_category: string, category: string, cost: double, margin: double, cost_total: double, profit: double],\n",
       " DataFrame[year: int, month: int, region: string, total_revenue: double, total_profit: double, transaction_count: bigint, avg_transaction_value: double],\n",
       " DataFrame[product: string, category: string, total_revenue: double, total_profit: double, total_quantity: bigint, avg_profit_per_sale: double, profit_margin: double])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_3_file_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "831a6c12-aeea-4bbd-b8e9-e3489bab7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# EXERCISE 4: DATA QUALITY CHECKS\n",
    "# ==========================================\n",
    "\n",
    "def exercise_4_data_quality():\n",
    "    \"\"\"Data quality monitoring and validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXERCISE 4: DATA QUALITY MONITORING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    spark = create_workshop_spark()\n",
    "    \n",
    "    try:\n",
    "        # Define schema for consistent reading\n",
    "        sales_schema = StructType([\n",
    "            StructField(\"transaction_id\", StringType(), True),\n",
    "            StructField(\"transaction_date\", StringType(), True),\n",
    "            StructField(\"product\", StringType(), True),\n",
    "            StructField(\"region\", StringType(), True),\n",
    "            StructField(\"sales_rep\", StringType(), True),\n",
    "            StructField(\"quantity\", IntegerType(), True),\n",
    "            StructField(\"unit_price\", DoubleType(), True),\n",
    "            StructField(\"total_amount\", DoubleType(), True),\n",
    "            StructField(\"customer_segment\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Read the sales data with explicit schema\n",
    "        sales_df = spark.read.csv(\"workshop_sales.csv\", header=True, schema=sales_schema)\n",
    "        \n",
    "        total_records = sales_df.count()\n",
    "        print(f\"📊 Total records: {total_records:,}\")\n",
    "        \n",
    "        # 1. Completeness checks\n",
    "        print(\"\\n🔍 Completeness Analysis:\")\n",
    "        for column in sales_df.columns:\n",
    "            null_count = sales_df.filter(F.col(column).isNull()).count()\n",
    "            completeness_rate = ((total_records - null_count) / total_records) * 100\n",
    "            print(f\"  • {column}: {completeness_rate:.1f}% complete ({null_count} nulls)\")\n",
    "        \n",
    "        # 2. Data validation\n",
    "        print(\"\\n✅ Data Validation:\")\n",
    "        \n",
    "        # Check for negative amounts\n",
    "        negative_amounts = sales_df.filter(F.col(\"total_amount\") < 0).count()\n",
    "        print(f\"  • Negative amounts: {negative_amounts} records\")\n",
    "        \n",
    "        # Check for invalid quantities\n",
    "        invalid_quantities = sales_df.filter(F.col(\"quantity\") <= 0).count()\n",
    "        print(f\"  • Invalid quantities: {invalid_quantities} records\")\n",
    "        \n",
    "        # Check amount calculation consistency\n",
    "        validation_df = sales_df \\\n",
    "            .withColumn(\"calculated_amount\", F.col(\"quantity\") * F.col(\"unit_price\")) \\\n",
    "            .withColumn(\"amount_diff\", F.abs(F.col(\"total_amount\") - F.col(\"calculated_amount\")))\n",
    "        \n",
    "        inconsistent_amounts = validation_df.filter(F.col(\"amount_diff\") > 0.01).count()\n",
    "        print(f\"  • Amount calculation inconsistencies: {inconsistent_amounts} records\")\n",
    "        \n",
    "        # 3. Outlier detection using percentiles\n",
    "        print(\"\\n📈 Outlier Detection:\")\n",
    "        \n",
    "        # Calculate percentiles\n",
    "        percentiles = sales_df.select(\n",
    "            F.expr(\"percentile_approx(total_amount, 0.25)\").alias(\"q1\"),\n",
    "            F.expr(\"percentile_approx(total_amount, 0.75)\").alias(\"q3\"),\n",
    "            F.avg(\"total_amount\").alias(\"mean\"),\n",
    "            F.stddev(\"total_amount\").alias(\"stddev\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # IQR method for outliers\n",
    "        iqr = percentiles[\"q3\"] - percentiles[\"q1\"]\n",
    "        lower_bound = percentiles[\"q1\"] - 1.5 * iqr\n",
    "        upper_bound = percentiles[\"q3\"] + 1.5 * iqr\n",
    "        \n",
    "        outliers = sales_df.filter(\n",
    "            (F.col(\"total_amount\") < lower_bound) | \n",
    "            (F.col(\"total_amount\") > upper_bound)\n",
    "        ).count()\n",
    "        \n",
    "        print(f\"  • Statistical outliers (IQR): {outliers} records\")\n",
    "        print(f\"  • Normal range: ${lower_bound:.2f} - ${upper_bound:.2f}\")\n",
    "        \n",
    "        # 4. Duplicate detection\n",
    "        print(\"\\n🔄 Duplicate Analysis:\")\n",
    "        \n",
    "        duplicate_ids = sales_df \\\n",
    "            .groupBy(\"transaction_id\") \\\n",
    "            .count() \\\n",
    "            .filter(F.col(\"count\") > 1) \\\n",
    "            .count()\n",
    "        \n",
    "        print(f\"  • Duplicate transaction IDs: {duplicate_ids}\")\n",
    "        \n",
    "        # 5. Data quality summary\n",
    "        print(\"\\n🎯 Data Quality Summary:\")\n",
    "        validity_rate = ((total_records - negative_amounts - invalid_quantities - inconsistent_amounts) / total_records) * 100\n",
    "        uniqueness_rate = ((total_records - duplicate_ids) / total_records) * 100\n",
    "        \n",
    "        print(f\"  • Validity Score: {validity_rate:.1f}%\")\n",
    "        print(f\"  • Uniqueness Score: {uniqueness_rate:.1f}%\")\n",
    "        print(f\"  • Outlier Rate: {(outliers/total_records)*100:.1f}%\")\n",
    "        \n",
    "        # Create quality report\n",
    "        quality_metrics = [\n",
    "            (\"validity_rate\", f\"{validity_rate:.2f}%\"),\n",
    "            (\"uniqueness_rate\", f\"{uniqueness_rate:.2f}%\"),\n",
    "            (\"outlier_rate\", f\"{(outliers/total_records)*100:.2f}%\"),\n",
    "            (\"total_records\", str(total_records))\n",
    "        ]\n",
    "        \n",
    "        quality_df = spark.createDataFrame(quality_metrics, [\"metric\", \"value\"])\n",
    "        quality_df.show()\n",
    "        \n",
    "        # Save quality report\n",
    "        quality_df.coalesce(1).write.mode(\"overwrite\").csv(\"output/data_quality_report\", header=True)\n",
    "        \n",
    "        print(\"✅ Exercise 4 completed successfully!\")\n",
    "        \n",
    "        return quality_df\n",
    "        \n",
    "    finally:\n",
    "        # spark.stop()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3b240cf-31c2-4a71-b7c2-7b3ac90c1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MAIN WORKSHOP RUNNER\n",
    "# ==========================================\n",
    "\n",
    "def run_complete_workshop():\n",
    "    \"\"\"Run the complete workshop with all exercises\"\"\"\n",
    "    print(\"🚀 COMPLETE PYSPARK WORKSHOP\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🎯 All function conflicts resolved!\")\n",
    "    print(\"🐳 Running in Docker environment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Exercise 1: Basic operations\n",
    "        print(\"\\n🎯 Starting Exercise 1...\")\n",
    "        exercise_1_basic_spark_operations()\n",
    "        \n",
    "        # Exercise 2: Joins and aggregations  \n",
    "        print(\"\\n🎯 Starting Exercise 2...\")\n",
    "        exercise_2_joins_and_aggregations()\n",
    "        \n",
    "        # Exercise 3: File operations\n",
    "        print(\"\\n🎯 Starting Exercise 3...\")\n",
    "        exercise_3_file_operations()\n",
    "        \n",
    "        # Exercise 4: Data quality\n",
    "        print(\"\\n🎯 Starting Exercise 4...\")\n",
    "        exercise_4_data_quality()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎉 WORKSHOP COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\n📁 Files created:\")\n",
    "        print(\"  • output/exercise1/ - Basic operations results\")\n",
    "        print(\"  • output/dept_summary/ - Department aggregations\")\n",
    "        print(\"  • output/employee_rankings/ - Employee rankings\")\n",
    "        print(\"  • output/monthly_sales/ - Monthly sales analysis\")\n",
    "        print(\"  • output/product_performance/ - Product performance\")\n",
    "        print(\"  • output/sales_partitioned/ - Partitioned data (Parquet)\")\n",
    "        print(\"  • output/data_quality_report/ - Data quality metrics\")\n",
    "        \n",
    "        print(\"\\n🎓 Concepts covered:\")\n",
    "        print(\"  ✅ DataFrames and RDDs\")\n",
    "        print(\"  ✅ Transformations vs Actions\")\n",
    "        print(\"  ✅ Joins and Aggregations\")\n",
    "        print(\"  ✅ Window Functions\")\n",
    "        print(\"  ✅ Data Partitioning\")\n",
    "        print(\"  ✅ Data Quality Monitoring\")\n",
    "        print(\"  ✅ File I/O (CSV, Parquet)\")\n",
    "        print(\"  ✅ Performance Optimization\")\n",
    "        \n",
    "        print(\"\\n🌐 While exercises were running, Spark UI was available at:\")\n",
    "        print(\"    http://localhost:4040\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error occurred: {str(e)}\")\n",
    "        print(\"Check the error message above for details.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7d07189-2578-4d2a-95f4-9dc9f0b23c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SIMPLE DEMO FOR TESTING\n",
    "# ==========================================\n",
    "\n",
    "def simple_demo():\n",
    "    \"\"\"Simple demo to test everything works\"\"\"\n",
    "    print(\"🧪 SIMPLE DEMO - Testing Setup\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    spark = create_workshop_spark()\n",
    "    \n",
    "    try:\n",
    "        # Create simple test data\n",
    "        data = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Carol\", 35)]\n",
    "        df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "        \n",
    "        print(\"📊 Test data:\")\n",
    "        df.show()\n",
    "        \n",
    "        # Simple operations\n",
    "        result = df.filter(F.col(\"age\") > 28).withColumn(\"age_plus_10\", F.col(\"age\") + 10)\n",
    "        \n",
    "        print(\"🔍 Filtered and enhanced:\")\n",
    "        result.show()\n",
    "        \n",
    "        print(\"✅ Demo successful! Spark is working correctly.\")\n",
    "        print(\"🌐 Check Spark UI at: http://localhost:4040\")\n",
    "        \n",
    "    finally:\n",
    "        spark.stop()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b61cbd9-5409-458f-86c7-3f12371dfb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Run simple demo (recommended first)\n",
      "2. Run complete workshop\n",
      "🚀 COMPLETE PYSPARK WORKSHOP\n",
      "============================================================\n",
      "🎯 All function conflicts resolved!\n",
      "🐳 Running in Docker environment\n",
      "============================================================\n",
      "\n",
      "🎯 Starting Exercise 1...\n",
      "\n",
      "============================================================\n",
      "EXERCISE 1: BASIC SPARK OPERATIONS\n",
      "============================================================\n",
      "📊 Creating test data...\n",
      "📋 Original data:\n",
      "+---+-------------+---+-----------+-----------------+\n",
      "| id|         name|age| department|            email|\n",
      "+---+-------------+---+-----------+-----------------+\n",
      "|  1|Alice Johnson| 28|Engineering|alice@company.com|\n",
      "|  2|    Bob Smith| 35|  Marketing|  bob@company.com|\n",
      "|  3|  Carol Davis| 42|Engineering|carol@company.com|\n",
      "|  4| David Wilson| 31|      Sales|david@company.com|\n",
      "|  5|    Eve Brown| 26|  Marketing|  eve@company.com|\n",
      "|  6| Frank Miller| 45|Engineering|frank@company.com|\n",
      "+---+-------------+---+-----------+-----------------+\n",
      "\n",
      "\n",
      "🔍 Filtering: age > 30\n",
      "+---+------------+---+-----------+-----------------+\n",
      "| id|        name|age| department|            email|\n",
      "+---+------------+---+-----------+-----------------+\n",
      "|  2|   Bob Smith| 35|  Marketing|  bob@company.com|\n",
      "|  3| Carol Davis| 42|Engineering|carol@company.com|\n",
      "|  4|David Wilson| 31|      Sales|david@company.com|\n",
      "|  6|Frank Miller| 45|Engineering|frank@company.com|\n",
      "+---+------------+---+-----------+-----------------+\n",
      "\n",
      "\n",
      "➕ Adding calculated columns:\n",
      "+---+------------+---+-----------+-----------------+-----------+-------------------+------------+\n",
      "| id|        name|age| department|            email|age_doubled|       age_category|email_domain|\n",
      "+---+------------+---+-----------+-----------------+-----------+-------------------+------------+\n",
      "|  2|   Bob Smith| 35|  Marketing|  bob@company.com|         70|         Mid-Career| company.com|\n",
      "|  3| Carol Davis| 42|Engineering|carol@company.com|         84|         Mid-Career| company.com|\n",
      "|  4|David Wilson| 31|      Sales|david@company.com|         62| Young Professional| company.com|\n",
      "|  6|Frank Miller| 45|Engineering|frank@company.com|         90|Senior Professional| company.com|\n",
      "+---+------------+---+-----------+-----------------+-----------+-------------------+------------+\n",
      "\n",
      "\n",
      "💾 Saving results...\n",
      "✅ Exercise 1 completed successfully!\n",
      "\n",
      "🎯 Starting Exercise 2...\n",
      "\n",
      "============================================================\n",
      "EXERCISE 2: JOINS AND AGGREGATIONS\n",
      "============================================================\n",
      "👥 Employees:\n",
      "+------+-----+-------+------+----------+\n",
      "|emp_id| name|dept_id|salary| hire_date|\n",
      "+------+-----+-------+------+----------+\n",
      "|     1|Alice|      1| 85000|2020-01-15|\n",
      "|     2|  Bob|      2| 75000|2019-03-20|\n",
      "|     3|Carol|      1| 95000|2018-07-10|\n",
      "|     4|David|      3| 65000|2021-02-28|\n",
      "|     5|  Eve|      2| 70000|2020-11-05|\n",
      "|     6|Frank|      1|105000|2017-09-12|\n",
      "|     7|Grace|      4| 80000|2019-06-18|\n",
      "|     8|Henry|      3| 68000|2021-08-03|\n",
      "+------+-----+-------+------+----------+\n",
      "\n",
      "🏢 Departments:\n",
      "+-------+-----------+----------+\n",
      "|dept_id|  dept_name|  division|\n",
      "+-------+-----------+----------+\n",
      "|      1|Engineering|Technology|\n",
      "|      2|  Marketing|  Business|\n",
      "|      3|      Sales|  Business|\n",
      "|      4|         HR|Operations|\n",
      "+-------+-----------+----------+\n",
      "\n",
      "\n",
      "🔗 Joining data...\n",
      "+-------+------+-----+------+----------+-----------+----------+\n",
      "|dept_id|emp_id| name|salary| hire_date|  dept_name|  division|\n",
      "+-------+------+-----+------+----------+-----------+----------+\n",
      "|      1|     1|Alice| 85000|2020-01-15|Engineering|Technology|\n",
      "|      1|     3|Carol| 95000|2018-07-10|Engineering|Technology|\n",
      "|      1|     6|Frank|105000|2017-09-12|Engineering|Technology|\n",
      "|      2|     2|  Bob| 75000|2019-03-20|  Marketing|  Business|\n",
      "|      2|     5|  Eve| 70000|2020-11-05|  Marketing|  Business|\n",
      "|      3|     4|David| 65000|2021-02-28|      Sales|  Business|\n",
      "|      3|     8|Henry| 68000|2021-08-03|      Sales|  Business|\n",
      "|      4|     7|Grace| 80000|2019-06-18|         HR|Operations|\n",
      "+-------+------+-----+------+----------+-----------+----------+\n",
      "\n",
      "\n",
      "📊 Department aggregations:\n",
      "+-----------+----------+------------+----------+--------------+----------+----------+------------------+\n",
      "|  dept_name|  division|total_salary|avg_salary|employee_count|max_salary|min_salary|avg_salary_rounded|\n",
      "+-----------+----------+------------+----------+--------------+----------+----------+------------------+\n",
      "|Engineering|Technology|      285000|   95000.0|             3|    105000|     85000|           95000.0|\n",
      "|  Marketing|  Business|      145000|   72500.0|             2|     75000|     70000|           72500.0|\n",
      "|      Sales|  Business|      133000|   66500.0|             2|     68000|     65000|           66500.0|\n",
      "|         HR|Operations|       80000|   80000.0|             1|     80000|     80000|           80000.0|\n",
      "+-----------+----------+------------+----------+--------------+----------+----------+------------------+\n",
      "\n",
      "\n",
      "🏆 Employee rankings within departments:\n",
      "+-----+-----------+------+-----------+-----------------+\n",
      "| name|  dept_name|salary|salary_rank|salary_percentile|\n",
      "+-----+-----------+------+-----------+-----------------+\n",
      "|Frank|Engineering|105000|          1|              0.0|\n",
      "|Carol|Engineering| 95000|          2|              0.5|\n",
      "|Alice|Engineering| 85000|          3|              1.0|\n",
      "|Grace|         HR| 80000|          1|              0.0|\n",
      "|  Bob|  Marketing| 75000|          1|              0.0|\n",
      "|  Eve|  Marketing| 70000|          2|              1.0|\n",
      "|Henry|      Sales| 68000|          1|              0.0|\n",
      "|David|      Sales| 65000|          2|              1.0|\n",
      "+-----+-----------+------+-----------+-----------------+\n",
      "\n",
      "\n",
      "💾 Saving results...\n",
      "✅ Exercise 2 completed successfully!\n",
      "\n",
      "🎯 Starting Exercise 3...\n",
      "\n",
      "============================================================\n",
      "EXERCISE 3: FILE OPERATIONS AND PARTITIONING\n",
      "============================================================\n",
      "📊 Creating workshop datasets...\n",
      "✅ Created datasets:\n",
      "  • workshop_sales.csv: 10,000 records\n",
      "  • workshop_customers.csv: 1,000 records\n",
      "  • workshop_products.csv: 5 records\n",
      "📖 Reading CSV files with explicit schemas...\n",
      "📊 Sales records: 10,000\n",
      "Sample sales data:\n",
      "+--------------+----------------+----------+------+---------+--------+----------+------------+----------------+\n",
      "|transaction_id|transaction_date|   product|region|sales_rep|quantity|unit_price|total_amount|customer_segment|\n",
      "+--------------+----------------+----------+------+---------+--------+----------+------------+----------------+\n",
      "|    TXN_000001|      2024-09-09|    Laptop| North|   Rep_20|       2|   1949.53|     3899.06|      Enterprise|\n",
      "|    TXN_000002|      2024-04-06|    Laptop| South|   Rep_10|       4|   1025.57|     4102.27|      Enterprise|\n",
      "|    TXN_000003|      2024-07-07|  Keyboard|  West|   Rep_10|       4|    851.49|     3405.94|        Consumer|\n",
      "|    TXN_000004|      2024-09-27|    Laptop| South|   Rep_08|       5|   1587.43|     7937.15|             SMB|\n",
      "|    TXN_000005|      2024-07-29|Headphones|  West|   Rep_01|       3|   1202.73|      3608.2|      Enterprise|\n",
      "+--------------+----------------+----------+------+---------+--------+----------+------------+----------------+\n",
      "\n",
      "\n",
      "🔄 Data transformations...\n",
      "\n",
      "🔗 Joining with product catalog...\n",
      "\n",
      "📊 Monthly sales analysis:\n",
      "+----+-----+------+-----------------+------------------+-----------------+---------------------+\n",
      "|year|month|region|    total_revenue|      total_profit|transaction_count|avg_transaction_value|\n",
      "+----+-----+------+-----------------+------------------+-----------------+---------------------+\n",
      "|2024|    1| South|718131.5800000001| 574951.5800000002|              230|   3122.3112173913046|\n",
      "|2024|    1|  West|684655.1299999999| 521095.1300000003|              240|    2852.729708333333|\n",
      "|2024|    1|  East|        659724.09| 499654.0899999999|              202|    3265.960841584158|\n",
      "|2024|    1| North|603331.6299999998|433611.62999999983|              214|   2819.3066822429896|\n",
      "|2024|    2|  East|673342.8399999999|500302.84000000026|              201|    3349.964378109452|\n",
      "|2024|    2| South|636364.2300000003|475154.22999999986|              204|   3119.4325000000017|\n",
      "|2024|    2| North|623335.2199999997|448005.21999999974|              206|    3025.899126213591|\n",
      "|2024|    2|  West|        600079.25| 466249.2500000001|              188|    3191.910904255319|\n",
      "|2024|    3|  West|679694.3499999997| 514874.3499999999|              234|    2904.676709401708|\n",
      "|2024|    3| South|        639346.06| 467216.0599999998|              188|   3400.7769148936172|\n",
      "|2024|    3|  East|592304.3500000002| 447174.3499999999|              207|   2861.3736714975857|\n",
      "|2024|    3| North|580383.8899999997| 434763.8899999997|              200|   2901.9194499999985|\n",
      "|2024|    4|  East|        714859.05| 535849.0500000003|              240|   2978.5793750000003|\n",
      "|2024|    4| South|        704560.31| 533690.3099999997|              201|   3505.2751741293537|\n",
      "|2024|    4|  West|686944.9999999999| 540004.9999999999|              215|   3195.0930232558135|\n",
      "|2024|    4| North|630208.0700000004| 465118.0700000001|              197|   3199.0257360406113|\n",
      "|2024|    5| North|        690716.79| 561286.7900000002|              204|   3385.8666176470592|\n",
      "|2024|    5| South|678337.7599999998|501307.76000000007|              222|   3055.5754954954946|\n",
      "|2024|    5|  East|676757.7099999996| 515077.7099999998|              217|    3118.699124423961|\n",
      "|2024|    5|  West|         621902.3|462842.29999999993|              223|    2788.799551569507|\n",
      "+----+-----+------+-----------------+------------------+-----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "🏆 Product performance:\n",
      "+----------+-----------+-----------------+------------------+--------------+-------------------+------------------+\n",
      "|   product|   category|    total_revenue|      total_profit|total_quantity|avg_profit_per_sale|     profit_margin|\n",
      "+----------+-----------+-----------------+------------------+--------------+-------------------+------------------+\n",
      "|   Monitor|    Display|       6498076.64|4619776.6400000015|          6261|  2257.955347018574|0.7109452374818408|\n",
      "|     Mouse|Accessories|6215339.070000001| 6095399.069999999|          5997|  3053.807149298597|0.9807025813637547|\n",
      "|Headphones|      Audio|6120274.950000008| 5520174.950000001|          6001| 2760.0874750000007| 0.901948849536571|\n",
      "|    Laptop|  Computing|6116389.369999998|1359589.3700000015|          5946|  692.9609429153933|0.2222862685408143|\n",
      "|  Keyboard|Accessories|6056011.750000014| 5755311.750000011|          6014| 2883.4227204408876|0.9503468598785327|\n",
      "+----------+-----------+-----------------+------------------+--------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "📂 Partitioning analysis:\n",
      "Original partitions: 1\n",
      "After repartitioning by region: 1\n",
      "\n",
      "💾 Saving partitioned data...\n",
      "✅ Exercise 3 completed successfully!\n",
      "\n",
      "🎯 Starting Exercise 4...\n",
      "\n",
      "============================================================\n",
      "EXERCISE 4: DATA QUALITY MONITORING\n",
      "============================================================\n",
      "📊 Total records: 10,000\n",
      "\n",
      "🔍 Completeness Analysis:\n",
      "  • transaction_id: 100.0% complete (0 nulls)\n",
      "  • transaction_date: 100.0% complete (0 nulls)\n",
      "  • product: 100.0% complete (0 nulls)\n",
      "  • region: 100.0% complete (0 nulls)\n",
      "  • sales_rep: 100.0% complete (0 nulls)\n",
      "  • quantity: 100.0% complete (0 nulls)\n",
      "  • unit_price: 100.0% complete (0 nulls)\n",
      "  • total_amount: 100.0% complete (0 nulls)\n",
      "  • customer_segment: 100.0% complete (0 nulls)\n",
      "\n",
      "✅ Data Validation:\n",
      "  • Negative amounts: 0 records\n",
      "  • Invalid quantities: 0 records\n",
      "  • Amount calculation inconsistencies: 3360 records\n",
      "\n",
      "📈 Outlier Detection:\n",
      "  • Statistical outliers (IQR): 75 records\n",
      "  • Normal range: $-3900.81 - $9648.51\n",
      "\n",
      "🔄 Duplicate Analysis:\n",
      "  • Duplicate transaction IDs: 0\n",
      "\n",
      "🎯 Data Quality Summary:\n",
      "  • Validity Score: 66.4%\n",
      "  • Uniqueness Score: 100.0%\n",
      "  • Outlier Rate: 0.8%\n",
      "+---------------+-------+\n",
      "|         metric|  value|\n",
      "+---------------+-------+\n",
      "|  validity_rate| 66.40%|\n",
      "|uniqueness_rate|100.00%|\n",
      "|   outlier_rate|  0.75%|\n",
      "|  total_records|  10000|\n",
      "+---------------+-------+\n",
      "\n",
      "✅ Exercise 4 completed successfully!\n",
      "\n",
      "============================================================\n",
      "🎉 WORKSHOP COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "📁 Files created:\n",
      "  • output/exercise1/ - Basic operations results\n",
      "  • output/dept_summary/ - Department aggregations\n",
      "  • output/employee_rankings/ - Employee rankings\n",
      "  • output/monthly_sales/ - Monthly sales analysis\n",
      "  • output/product_performance/ - Product performance\n",
      "  • output/sales_partitioned/ - Partitioned data (Parquet)\n",
      "  • output/data_quality_report/ - Data quality metrics\n",
      "\n",
      "🎓 Concepts covered:\n",
      "  ✅ DataFrames and RDDs\n",
      "  ✅ Transformations vs Actions\n",
      "  ✅ Joins and Aggregations\n",
      "  ✅ Window Functions\n",
      "  ✅ Data Partitioning\n",
      "  ✅ Data Quality Monitoring\n",
      "  ✅ File I/O (CSV, Parquet)\n",
      "  ✅ Performance Optimization\n",
      "\n",
      "🌐 While exercises were running, Spark UI was available at:\n",
      "    http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ENTRY POINT\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose an option:\")\n",
    "    print(\"1. Run simple demo (recommended first)\")\n",
    "    print(\"2. Run complete workshop\")\n",
    "    \n",
    "    # For automatic execution, just run the complete workshop\n",
    "    run_complete_workshop()\n",
    "    \n",
    "    # Uncomment this line if you want to run just the simple demo first:\n",
    "    # simple_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5bd19-85af-4bdd-ae53-c186388e55d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f425636-896e-471b-af6d-9c4eff7ca87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
